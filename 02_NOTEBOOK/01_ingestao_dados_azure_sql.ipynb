{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba355bb1-8f54-4add-9251-ca4cd096f204",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## [PRÉ REQUISITO] Atividades Desenvolvidas Fora do Ambiente Databricks\n",
    "\n",
    "|Sequência|Ação|Detalhamento\n",
    "|---|---|---|\n",
    "|SEQ-01|Provisionamento do Azure SQL|Provisionamento de um banco de dados no Azure|\n",
    "|SEQ-02|Configuração do Ambiente Azure|Criação de toda a estrutura de tabelas e dados|\n",
    "\n",
    "\n",
    "## Atividades Desenvolvidas no Notebook - 01_ingestao_dados_azure_sql\n",
    "\n",
    "|Sequência|Ação|Detalhamento\n",
    "|---|---|---|\n",
    "|SEQ-01 / SEQ-02|Configuração de Biblioteca|Instalação da Biblioteca \"sqlalchemy\"|\n",
    "|SEQ-03|Selação das Tabelas|Identificar quais serão as tabelas usadas durante o processo de ingestão|\n",
    "|SEQ-04|Extração dos Dados do Azure SQL|Coleta dos Dados do Azure SQL|\n",
    "|SEQ-05|Persistir os Dados em Parquet|Os dados deverão ser persistidos no diretório de cada tabela em formato parquet.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d1c97f-2470-4a4e-95f0-b09ce013bb57",
     "showTitle": true,
     "title": "01 - Configuração da Biblioteca sqlalchemy"
    }
   },
   "outputs": [],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5472f1b1-a2c4-4998-b588-4826f5a88178",
     "showTitle": true,
     "title": "02 - Importação das Bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import* #StructType, StructField, StringType\n",
    "from pyspark.sql.functions import lit, col\n",
    "import pyodbc\n",
    "from sqlalchemy import __version__ as sa_version, create_engine, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960be4a2-c669-4c63-897e-a0242edc2c16",
     "showTitle": true,
     "title": "03 - Mapear Todas as Tabelas Existentes no Azure SQL"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"jdbc\") \\\n",
    ".option(\"url\", \"jdbc:sqlserver://srv-db-treinamentosql.database.windows.net:1433;database=srv-db-treinamentosql\") \\\n",
    ".option(\"query\", \"SELECT NAME AS Nome_Tabela FROM sys.tables\") \\\n",
    ".option(\"user\", \"\") \\\n",
    ".option(\"password\", \"\") \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf05b1cd-63be-4761-bd68-07891e4d57e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7f0f82-1430-453e-89c2-494d60e1aac5",
     "showTitle": true,
     "title": "04 - Coletar os Dados das Tabelas Existentes no Azure SQL e Salvando em Parquet na Caamda Landing_Zone"
    }
   },
   "outputs": [],
   "source": [
    "lista_tabelas = df.collect()\n",
    "\n",
    "for tabela in lista_tabelas:\n",
    "\n",
    "    print(tabela['Nome_Tabela'])\n",
    "\n",
    "    diretorio_parquet = \"dbfs:/FileStore/tables/landing_zone/{0}\".format(tabela['Nome_Tabela'])\n",
    "\n",
    "    df = (spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:sqlserver://srv-db-treinamentosql.database.windows.net:1433;database=srv-db-treinamentosql\")\n",
    "    .option(\"query\", \"SELECT * FROM dbo.{0}\".format(tabela['Nome_Tabela']))\n",
    "    .option(\"user\", \"usr-srv-treinamentosql\")\n",
    "    .option(\"password\", \"42250000Gl!.\")\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "    \n",
    "    (df.write\n",
    "    .format('parquet')\n",
    "    .mode(\"overwrite\")\n",
    "    .save(diretorio_parquet)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1934667511695067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestao_dados_azure_sql",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
