{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba355bb1-8f54-4add-9251-ca4cd096f204",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## [PRÉ REQUISITO] Atividades Desenvolvidas Fora do Ambiente Databricks\n",
    "\n",
    "|Sequência|Ação|Detalhamento\n",
    "|---|---|---|\n",
    "|SEQ-01|Provisionamento do Azure SQL|Provisionamento de um banco de dados no Azure|\n",
    "|SEQ-02|Configuração do Ambiente Azure|Criação de toda a estrutura de tabelas e dados|\n",
    "\n",
    "\n",
    "## Atividades Desenvolvidas no Notebook - 01_ingestao_dados_azure_sql\n",
    "\n",
    "|Sequência|Ação|Detalhamento\n",
    "|---|---|---|\n",
    "|SEQ-01 / SEQ-02|Configuração de Biblioteca|Instalação da Biblioteca \"sqlalchemy\"|\n",
    "|SEQ-03|Consumindo Arquivo JSON|Arquivo com as credenciais de acesso ao Azure SQL Database|\n",
    "|SEQ-04|Selação das Tabelas|Identificar quais serão as tabelas usadas durante o processo de ingestão|\n",
    "|SEQ-05|Extração dos Dados do Azure SQL|Coleta dos Dados do Azure SQL|\n",
    "|SEQ-06|Persistir os Dados em Parquet|Os dados deverão ser persistidos no diretório de cada tabela em formato parquet.|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d1c97f-2470-4a4e-95f0-b09ce013bb57",
     "showTitle": true,
     "title": "SEQ-01 - Configuração da Biblioteca sqlalchemy"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting sqlalchemy\n  Downloading SQLAlchemy-2.0.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nCollecting typing-extensions>=4.2.0\n  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-3.0.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (610 kB)\nInstalling collected packages: typing-extensions, greenlet, sqlalchemy\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-056af41b-a629-435a-9480-7da83445abc2\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed greenlet-3.0.1 sqlalchemy-2.0.23 typing-extensions-4.8.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5472f1b1-a2c4-4998-b588-4826f5a88178",
     "showTitle": true,
     "title": "SEQ-02 - Importação das Bibliotecas"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sqlalchemy import __version__ as sa_version, create_engine, text\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e59b0d4-1188-4a31-93ff-0c20316e403f",
     "showTitle": true,
     "title": "SEQ-03 - Consumindo Arquivo JSON"
    }
   },
   "outputs": [],
   "source": [
    "dfjson =  pd.read_json(\"https://raw.githubusercontent.com/dbaassists/Projeto_BI_Zero_TO_DW/main/04_ARQUIVO_CONFIG/config_azure_sql.json?token=GHSAT0AAAAAACHUV6QKZTZTLTBSZTO7M2FUZKNGCSA\")\n",
    "\n",
    "server = dfjson['Config']['server']\n",
    "database = dfjson['Config']['database']\n",
    "username = dfjson['Config']['username']\n",
    "password = dfjson['Config']['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "960be4a2-c669-4c63-897e-a0242edc2c16",
     "showTitle": true,
     "title": "SEQ-04 - Mapear Todas as Tabelas Existentes no Azure SQL"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    ".format(\"jdbc\") \\\n",
    ".option(\"url\", f\"jdbc:sqlserver://{server};database={database}\") \\\n",
    ".option(\"query\", \"\"\"SELECT s.name + '.' +  t.NAME AS Nome_Tabela \n",
    "                        FROM sys.tables t\n",
    "                        INNER JOIN sys.schemas s\n",
    "                        ON t.schema_id = s.schema_id\"\"\") \\\n",
    ".option(\"user\", f\"{username}\") \\\n",
    ".option(\"password\", f\"{password}\") \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf05b1cd-63be-4761-bd68-07891e4d57e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Nome_Tabela</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "Nome_Tabela",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7f0f82-1430-453e-89c2-494d60e1aac5",
     "showTitle": true,
     "title": "SEQ-05 - Coletar os Dados das Tabelas Existentes no Azure SQL e Salvando em Parquet na Caamda Landing_Zone"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbo.TB_FORMA_PAGAMENTO\ndbo.TB_CATEGORIA_PRODUTO\ndbo.TB_PRODUTO\ndbo.TB_CLIENTE\ndbo.TB_VENDEDOR\ndbo.TB_LOJA\ndbo.TB_VENDA\ndbo.TB_ITEM_VENDA\n"
     ]
    }
   ],
   "source": [
    "lista_tabelas = df.collect()\n",
    "\n",
    "for tabela in lista_tabelas:\n",
    "\n",
    "    print(tabela['Nome_Tabela'])\n",
    "\n",
    "    diretorio_parquet = \"dbfs:/FileStore/tables/landing_zone/{0}\".format(tabela['Nome_Tabela'])\n",
    "\n",
    "    df = (spark.read\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", f\"jdbc:sqlserver://{server};database={database}\") \\\n",
    "    .option(\"query\", \"SELECT * FROM {0}\".format(tabela['Nome_Tabela']))\n",
    "    .option(\"user\", f\"{username}\") \\\n",
    "    .option(\"password\", f\"{password}\") \\\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "    \n",
    "    (df.write\n",
    "    .format('parquet')\n",
    "    .mode(\"overwrite\")\n",
    "    .save(diretorio_parquet)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1934667511695067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingestao_dados_azure_sql",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
